{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D Image to 3D Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Material Properties From json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-21 22:35:13.873888: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from PIL import Image\n",
    "from PIL import ImageOps\n",
    "import numpy as np\n",
    "import trimesh\n",
    "import open3d as o3d\n",
    "import tensorflow as tf\n",
    "\n",
    "with open('resized_img_processed_model_mapping.json', 'r') as f:\n",
    "    img_to_mod_map = json.load(f)\n",
    "\n",
    "with open('material_properties.json', 'r') as f:\n",
    "    material_properties = json.load(f)\n",
    "\n",
    "\n",
    "def load_preprocess_img(img_path):\n",
    "    \n",
    "    img = Image.open(img_path)\n",
    "\n",
    "    if img.mode != 'RGB':\n",
    "        # print(f\"Converting grayscale to RGB for: {img_path}\")\n",
    "        img = ImageOps.grayscale(img)\n",
    "        img = ImageOps.colorize(img, black=\"black\", white=\"white\")\n",
    "\n",
    "    img_array = np.array(img)\n",
    "    img_array = img_array / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    return img_array\n",
    "\n",
    "def simplify_mesh(mesh, target_vertices=1024):\n",
    "\n",
    "    open3d_mesh = o3d.geometry.TriangleMesh(\n",
    "        vertices=o3d.utility.Vector3dVector(mesh.vertices),\n",
    "        triangles=o3d.utility.Vector3iVector(mesh.faces)\n",
    "    )\n",
    "\n",
    "    simplified_mesh = open3d_mesh.simplify_quadric_decimation(target_vertices)\n",
    "    simplified_trimesh = trimesh.Trimesh(\n",
    "        vertices=np.asarray(simplified_mesh.vertices),\n",
    "        faces=np.asarray(simplified_mesh.triangles)\n",
    "    )\n",
    "\n",
    "    return simplified_trimesh\n",
    "\n",
    "def upsample_mesh(mesh, target_vertices=1024):\n",
    "    sampled_points, _ = trimesh.sample.sample_surface_even(mesh, target_vertices)\n",
    "    if len(sampled_points) < target_vertices:\n",
    "        padding = np.zeros((target_vertices - len(sampled_points), 3))\n",
    "        return np.vstack([sampled_points, padding])\n",
    "    return sampled_points\n",
    "\n",
    "\n",
    "def load_3d_model(model_path, target_vertices=1500):\n",
    "    mesh = trimesh.load(model_path)\n",
    "\n",
    "    # print(f\"In load_3d: len(mesh.vertices): {len(mesh.vertices)}\")\n",
    "    if len(mesh.vertices) > target_vertices:\n",
    "        simplified_mesh = simplify_mesh(mesh, target_vertices)\n",
    "        # print(f\"Simplify mesh: len(mesh.vertices): {len(mesh.vertices)}\")\n",
    "        return simplified_mesh\n",
    "\n",
    "    elif len(mesh.vertices) < target_vertices:\n",
    "        upsampled_mesh = upsample_mesh(mesh, target_vertices)\n",
    "        # print(f\"Upsample mesh: len(mesh.vertices): {len(mesh.vertices)}\")\n",
    "        return upsampled_mesh\n",
    "    \n",
    "    return mesh\n",
    "\n",
    "\n",
    "def get_material_prop(img_path, img_to_mod_map, material_properties):\n",
    "    # print(img_path)\n",
    "    model_path = img_to_mod_map.get(img_path, None)\n",
    "    # print(f\"Processing image: {img_path} with mesh: {model_path}\")\n",
    "\n",
    "    if model_path is None:\n",
    "        raise ValueError(f\"No model found for img: {img_path}\")\n",
    "\n",
    "    material_path = model_path.replace('simple_normal_model.obj', 'model.mtl')\n",
    "    material_path = material_path.replace('../model/', '')\n",
    "    materials = material_properties.get(material_path, None)\n",
    "    if materials is None:\n",
    "        raise ValueError(f\"No materials found for model: {material_path}\")\n",
    "    return materials\n",
    "\n",
    "\n",
    "def normalize_materials(material):\n",
    "    max_shine = 1000\n",
    "\n",
    "    normalized_material = {\n",
    "        'Kd': material.get('diffuse', [1.0, 1.0, 1.0]),\n",
    "        'Ks': material.get('specular', [0.0, 0.0, 0.0]),\n",
    "        'Ns': material.get('shininess', 96.078431) / max_shine,\n",
    "        'Ka': material.get('ambient', [0.0, 0.0, 0.0]),\n",
    "        'd': material.get('transparency', 1.0),\n",
    "        'illumination': material.get('illumination', 2)\n",
    "    }\n",
    "\n",
    "    # Flatten the normalized material into a list for easier processing\n",
    "    flattened_material = (\n",
    "        normalized_material['Kd'] + \n",
    "        normalized_material['Ks'] + \n",
    "        [normalized_material['Ns']] + \n",
    "        normalized_material['Ka'] + \n",
    "        [normalized_material['d'], normalized_material['illumination']]\n",
    "    )\n",
    "    \n",
    "    return flattened_material\n",
    "\n",
    "\n",
    "def preprocess_image_with_material(img_path, img_to_mod_map, material_properties):\n",
    "    img = load_preprocess_img(img_path)\n",
    "\n",
    "    model_path = img_to_mod_map.get(img_path)\n",
    "    mesh = load_3d_model(model_path, target_vertices=1500)\n",
    "\n",
    "    materials = get_material_prop(img_path, img_to_mod_map, material_properties)\n",
    "    normalized_materials = normalize_materials(materials)\n",
    "\n",
    "    return img, mesh, normalized_materials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, img_paths, img_to_mod_map, material_properties, batch_size=16, dim=(256, 256, 3), augment=False):\n",
    "        self.img_paths = img_paths\n",
    "        self.img_to_mod_map = img_to_mod_map\n",
    "        self.material_properties = material_properties\n",
    "        self.batch_size = batch_size\n",
    "        self.dim = dim\n",
    "        self.augment = augment\n",
    "\n",
    "        # Image data augmentation\n",
    "        self.image_datagen = ImageDataGenerator(\n",
    "            rotation_range=20,\n",
    "            width_shift_range=0.1,\n",
    "            height_shift_range=0.1,\n",
    "            zoom_range=0.1,\n",
    "            brightness_range=[0.8, 1.2],\n",
    "            fill_mode='nearest'\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.img_paths) / self.batch_size))\n",
    "\n",
    "    def pad_or_trunc_mesh(self, mesh, target_vertices=1500):\n",
    "        vertices = np.array(mesh)\n",
    "        # print(f\"Vertices shape before padding: {vertices.shape}\")\n",
    "\n",
    "        if vertices.shape[0] > target_vertices:\n",
    "            return vertices[:target_vertices, :]\n",
    "        elif vertices.shape[0] < target_vertices:\n",
    "            padding = np.zeros((target_vertices - vertices.shape[0], vertices.shape[1]))\n",
    "            return np.vstack([vertices, padding])\n",
    "        else:\n",
    "            return vertices\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_img_paths = self.img_paths[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        if len(batch_img_paths) == 0:\n",
    "            raise ValueError(f\"Batch {index} is empty. Skipping...\")\n",
    "\n",
    "        # print(f\"Batch index {index} size: {len(batch_img_paths)}\")\n",
    "\n",
    "        imgs = []\n",
    "        materials = []\n",
    "        meshes = []\n",
    "\n",
    "        for img_path in batch_img_paths:\n",
    "            img = load_preprocess_img(img_path)\n",
    "\n",
    "            # Only squeeze if the image has 4 dimensions\n",
    "            # print(img.shape)\n",
    "            if len(img.shape) == 4 and img.shape[0] == 1:\n",
    "                img = np.squeeze(img, axis=0)  # Remove batch dimension if present\n",
    "\n",
    "            if len(img.shape) == 2: \n",
    "                # print(f\"Converting grayscale to RGB for: {img_path}\")\n",
    "                img = np.stack([img] * 3, axis=-1) \n",
    "\n",
    "            if self.augment:\n",
    "                # print(img.shape)\n",
    "                img = self.image_datagen.random_transform(img)\n",
    "            \n",
    "            img = np.expand_dims(img, axis=0)\n",
    "            imgs.append(img)\n",
    "\n",
    "            material = get_material_prop(img_path, self.img_to_mod_map, self.material_properties)\n",
    "            normalized_material = normalize_materials(material)\n",
    "            materials.append(normalized_material)\n",
    "\n",
    "            model_path = self.img_to_mod_map.get(img_path)\n",
    "            mesh = load_3d_model(model_path)\n",
    "            # print(f\"Mesh type: {type(mesh)} for model: {model_path}\")\n",
    "\n",
    "            if index == 8:\n",
    "                print(f\"Batch 8 - Image Path: {img_path}, Model Path: {model_path}\")\n",
    "\n",
    "            # Additional logging for mesh loading issues\n",
    "            if hasattr(mesh, 'vertices'):\n",
    "                vertices_before = len(mesh.vertices)\n",
    "                # print(f\"Batch {index} - Image Path: {img_path}, Model Path: {model_path}\")\n",
    "                # print(f\"(If hasattr(mesh, 'vertices') | Vertices shape before padding: {vertices_before}\")\n",
    "                # if vertices_before < 1024:\n",
    "                #     # print(f\"Warning: Mesh for {model_path} has fewer vertices ({vertices_before}) than required. Adding padding.\")\n",
    "                # if vertices_before < 500:  # Log very small meshes\n",
    "                #     # print(f\"Warning: Mesh for {model_path} has abnormally few vertices ({vertices_before}).\")\n",
    "                padded_mesh = self.pad_or_trunc_mesh(mesh.vertices)\n",
    "            elif isinstance(mesh, np.ndarray) or isinstance(mesh, trimesh.caching.TrackedArray):\n",
    "                vertices_before = len(mesh)\n",
    "                # print(f\"Batch {index} - Image Path: {img_path}, Model Path: {model_path}\")\n",
    "                # print(f\"(If isinstance(mesh, np.ndarray)... | Vertices shape before padding: {vertices_before}\")\n",
    "                # if vertices_before < 500:  # Log very small meshes\n",
    "                #     print(f\"Warning: Mesh for {model_path} has abnormally few vertices ({vertices_before}).\")\n",
    "                # if vertices_before < 1024:\n",
    "                #     print(f\"Warning: Mesh for {model_path} has fewer vertices ({vertices_before}) than required. Adding padding.\")\n",
    "                padded_mesh = self.pad_or_trunc_mesh(mesh)\n",
    "            else:\n",
    "                print(f\"Warning: No vertices found in model: {model_path}. Skipping.\")\n",
    "                padded_mesh = np.zeros((1024, 3))\n",
    "\n",
    "\n",
    "            # if index == 8:\n",
    "            #     print(f\"Batch 8 - Padded Mesh Shape: {padded_mesh.shape}\")\n",
    "                \n",
    "            # print(f\"Final mesh shape: {padded_mesh.shape}\")\n",
    "            assert padded_mesh.shape[0] == 1500, f\"Unexpected vertex count: {padded_mesh.shape[0]} for mesh in batch {index}\"\n",
    "            meshes.append(padded_mesh)\n",
    "\n",
    "        imgs = np.vstack(imgs)\n",
    "        materials = np.array(materials, dtype=np.float32)\n",
    "        meshes = np.array(meshes, dtype=np.float32)\n",
    "\n",
    "        imgs_tensor = tf.convert_to_tensor(imgs, dtype=tf.float32)\n",
    "        materials_tensor = tf.convert_to_tensor(materials, dtype=tf.float32)\n",
    "        meshes_tensor = tf.convert_to_tensor(meshes, dtype=tf.float32)\n",
    "\n",
    "        return (imgs_tensor, materials_tensor), meshes_tensor\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.img_paths)\n",
    "        # print(\"Shuffled\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D, Concatenate, Reshape, BatchNormalization, ReLU\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import ResNet50V2\n",
    "\n",
    "image_input = Input(shape=(256, 256, 3), name='image_input')\n",
    "resnet_base = ResNet50V2(weights='imagenet', include_top=False, input_tensor=image_input)\n",
    "\n",
    "x = resnet_base.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(512)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = ReLU()(x)\n",
    "\n",
    "material_input = Input(shape=(12,), name='material_input')\n",
    "material_dense = Dense(64)(material_input)\n",
    "material_dense = BatchNormalization()(material_dense)\n",
    "material_dense = ReLU()(material_dense)\n",
    "\n",
    "combined = Concatenate()([x, material_dense])\n",
    "\n",
    "z = Dense(256)(combined)\n",
    "z = BatchNormalization()(z)\n",
    "z = ReLU()(z)\n",
    "z = Dense(512, activation='relu')(z)\n",
    "z = BatchNormalization()(z)\n",
    "z = ReLU()(z)\n",
    "\n",
    "output = Dense(1500 * 3, activation='linear', name='output')(z)\n",
    "output_reshaped = Reshape((1500, 3))(output)\n",
    "\n",
    "model = Model(inputs=[image_input, material_input], outputs=output_reshaped)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8521 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "only got 1197/1500 samples!\n",
      "only got 1405/1500 samples!\n",
      "only got 1119/1500 samples!\n",
      "only got 1212/1500 samples!\n",
      "only got 1471/1500 samples!\n",
      "only got 1407/1500 samples!\n",
      "only got 1273/1500 samples!\n",
      "only got 1042/1500 samples!\n",
      "only got 1149/1500 samples!\n",
      "only got 1380/1500 samples!\n",
      "only got 1197/1500 samples!\n",
      "only got 1254/1500 samples!\n",
      "only got 1428/1500 samples!\n",
      "only got 1368/1500 samples!\n",
      "only got 1135/1500 samples!\n",
      "only got 896/1500 samples!\n",
      "only got 1319/1500 samples!\n",
      "only got 866/1500 samples!\n",
      "only got 1415/1500 samples!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/karnerdavid/anaconda3/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n",
      "only got 1319/1500 samples!\n",
      "only got 1460/1500 samples!\n",
      "only got 858/1500 samples!\n",
      "only got 1060/1500 samples!\n",
      "only got 887/1500 samples!\n",
      "only got 1163/1500 samples!\n",
      "only got 1165/1500 samples!\n",
      "only got 854/1500 samples!\n",
      "only got 1292/1500 samples!\n",
      "only got 1141/1500 samples!\n",
      "only got 841/1500 samples!\n",
      "only got 1289/1500 samples!\n",
      "only got 1176/1500 samples!\n",
      "only got 1288/1500 samples!\n",
      "only got 1437/1500 samples!\n",
      "only got 1179/1500 samples!\n",
      "only got 1195/1500 samples!\n",
      "only got 1275/1500 samples!\n",
      "only got 1103/1500 samples!\n",
      "only got 1176/1500 samples!\n",
      "only got 1187/1500 samples!\n",
      "only got 1388/1500 samples!\n",
      "only got 1142/1500 samples!\n",
      "only got 1324/1500 samples!\n",
      "only got 1307/1500 samples!\n",
      "only got 1298/1500 samples!\n",
      "only got 1074/1500 samples!\n",
      "only got 899/1500 samples!\n",
      "only got 1167/1500 samples!\n",
      "only got 1310/1500 samples!\n",
      "only got 1105/1500 samples!\n",
      "only got 1106/1500 samples!\n",
      "only got 710/1500 samples!\n",
      "only got 1466/1500 samples!\n",
      "only got 1334/1500 samples!\n",
      "only got 1363/1500 samples!\n",
      "only got 1473/1500 samples!\n",
      "only got 939/1500 samples!\n",
      "only got 1328/1500 samples!\n",
      "only got 1094/1500 samples!\n",
      "only got 1380/1500 samples!\n",
      "only got 928/1500 samples!\n",
      "only got 1077/1500 samples!\n",
      "only got 1227/1500 samples!\n",
      "only got 1104/1500 samples!\n",
      "only got 863/1500 samples!\n",
      "only got 1421/1500 samples!\n",
      "only got 1234/1500 samples!\n",
      "only got 1101/1500 samples!\n",
      "only got 1222/1500 samples!\n",
      "only got 1195/1500 samples!\n",
      "only got 1090/1500 samples!\n",
      "only got 1388/1500 samples!\n",
      "only got 1419/1500 samples!\n",
      "only got 1215/1500 samples!\n",
      "only got 775/1500 samples!\n",
      "2024-09-21 22:38:51.045975: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:450] ShuffleDatasetV3:8: Filling up shuffle buffer (this may take a while): 7 of 8\n",
      "only got 1398/1500 samples!\n",
      "only got 1485/1500 samples!\n",
      "only got 1490/1500 samples!\n",
      "only got 1074/1500 samples!\n",
      "only got 1426/1500 samples!\n",
      "only got 905/1500 samples!\n",
      "only got 1244/1500 samples!\n",
      "2024-09-21 22:38:52.497124: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:480] Shuffle buffer filled.\n",
      "only got 1490/1500 samples!\n",
      "only got 1482/1500 samples!\n",
      "only got 1465/1500 samples!\n",
      "only got 1495/1500 samples!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  1/372\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6:59:35\u001b[0m 68s/step - loss: 0.1839"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "only got 1328/1500 samples!\n",
      "only got 1416/1500 samples!\n",
      "only got 1393/1500 samples!\n",
      "only got 1109/1500 samples!\n",
      "only got 1076/1500 samples!\n",
      "only got 1486/1500 samples!\n",
      "only got 932/1500 samples!\n",
      "only got 1335/1500 samples!\n",
      "only got 1185/1500 samples!\n",
      "only got 1048/1500 samples!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  2/372\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:09:09\u001b[0m 11s/step - loss: 0.1721"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "only got 904/1500 samples!\n",
      "only got 1139/1500 samples!\n",
      "only got 1121/1500 samples!\n",
      "only got 1323/1500 samples!\n",
      "only got 876/1500 samples!\n",
      "only got 1086/1500 samples!\n",
      "only got 1469/1500 samples!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  3/372\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:08:48\u001b[0m 11s/step - loss: 0.1653"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "only got 1213/1500 samples!\n",
      "only got 1483/1500 samples!\n",
      "only got 1445/1500 samples!\n",
      "only got 1331/1500 samples!\n",
      "only got 853/1500 samples!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  4/372\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:11:30\u001b[0m 12s/step - loss: 0.1598"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "only got 1251/1500 samples!\n",
      "only got 1460/1500 samples!\n",
      "only got 1401/1500 samples!\n",
      "only got 1343/1500 samples!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  5/372\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:16:56\u001b[0m 13s/step - loss: 0.1558"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "only got 1487/1500 samples!\n",
      "only got 1177/1500 samples!\n",
      "only got 1221/1500 samples!\n",
      "only got 1059/1500 samples!\n",
      "only got 1058/1500 samples!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  6/372\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:17:17\u001b[0m 13s/step - loss: 0.1519"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "only got 1017/1500 samples!\n",
      "only got 1176/1500 samples!\n",
      "only got 1351/1500 samples!\n",
      "only got 1116/1500 samples!\n",
      "only got 1238/1500 samples!\n",
      "only got 1183/1500 samples!\n",
      "only got 1197/1500 samples!\n",
      "only got 1192/1500 samples!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  7/372\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:16:07\u001b[0m 13s/step - loss: 0.1482"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "only got 1179/1500 samples!\n",
      "only got 1194/1500 samples!\n",
      "only got 1493/1500 samples!\n",
      "only got 1455/1500 samples!\n",
      "only got 1447/1500 samples!\n",
      "only got 1388/1500 samples!\n",
      "only got 1492/1500 samples!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  8/372\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:14:38\u001b[0m 12s/step - loss: 0.1447"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "only got 1119/1500 samples!\n",
      "only got 1181/1500 samples!\n",
      "only got 822/1500 samples!\n",
      "only got 1423/1500 samples!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m  9/372\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:14:45\u001b[0m 12s/step - loss: 0.1415"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "only got 1208/1500 samples!\n",
      "only got 1488/1500 samples!\n",
      "only got 824/1500 samples!\n",
      "only got 1321/1500 samples!\n",
      "only got 1400/1500 samples!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 10/372\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:14:51\u001b[0m 12s/step - loss: 0.1385"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "only got 1476/1500 samples!\n",
      "only got 1086/1500 samples!\n",
      "only got 1241/1500 samples!\n",
      "only got 1408/1500 samples!\n",
      "only got 1442/1500 samples!\n",
      "only got 1371/1500 samples!\n",
      "only got 1360/1500 samples!\n",
      "only got 1491/1500 samples!\n",
      "only got 901/1500 samples!\n",
      "only got 721/1500 samples!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 11/372\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:13:40\u001b[0m 12s/step - loss: 0.1357"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "only got 1178/1500 samples!\n",
      "only got 1458/1500 samples!\n",
      "only got 1456/1500 samples!\n",
      "only got 1218/1500 samples!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 12/372\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:12:57\u001b[0m 12s/step - loss: 0.1331"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "only got 1039/1500 samples!\n",
      "only got 1354/1500 samples!\n",
      "only got 1310/1500 samples!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 13/372\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:12:20\u001b[0m 12s/step - loss: 0.1306"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "only got 1439/1500 samples!\n",
      "only got 1470/1500 samples!\n",
      "only got 1087/1500 samples!\n",
      "only got 1321/1500 samples!\n",
      "only got 1370/1500 samples!\n",
      "only got 1163/1500 samples!\n",
      "only got 853/1500 samples!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 14/372\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:12:34\u001b[0m 12s/step - loss: 0.1283"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "only got 744/1500 samples!\n",
      "only got 1007/1500 samples!\n",
      "only got 1326/1500 samples!\n",
      "only got 1031/1500 samples!\n",
      "only got 1061/1500 samples!\n",
      "only got 1219/1500 samples!\n",
      "only got 842/1500 samples!\n",
      "only got 1232/1500 samples!\n",
      "only got 1499/1500 samples!\n",
      "only got 1278/1500 samples!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 15/372\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:13:12\u001b[0m 12s/step - loss: 0.1262"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "only got 1426/1500 samples!\n",
      "only got 1234/1500 samples!\n",
      "only got 1128/1500 samples!\n",
      "only got 1488/1500 samples!\n",
      "only got 1139/1500 samples!\n",
      "only got 986/1500 samples!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 16/372\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:12:11\u001b[0m 12s/step - loss: 0.1241"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "only got 1355/1500 samples!\n",
      "only got 1102/1500 samples!\n",
      "only got 853/1500 samples!\n",
      "only got 1032/1500 samples!\n",
      "only got 877/1500 samples!\n",
      "only got 1146/1500 samples!\n",
      "only got 859/1500 samples!\n",
      "only got 1460/1500 samples!\n",
      "only got 1347/1500 samples!\n",
      "only got 1070/1500 samples!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 17/372\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:11:13\u001b[0m 12s/step - loss: 0.1222"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "only got 1067/1500 samples!\n",
      "only got 1112/1500 samples!\n",
      "only got 1271/1500 samples!\n",
      "only got 1168/1500 samples!\n",
      "only got 1336/1500 samples!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 18/372\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:10:18\u001b[0m 12s/step - loss: 0.1204"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "only got 856/1500 samples!\n",
      "only got 888/1500 samples!\n",
      "only got 1318/1500 samples!\n",
      "only got 1087/1500 samples!\n",
      "only got 1379/1500 samples!\n",
      "only got 1212/1500 samples!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 19/372\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:10:30\u001b[0m 12s/step - loss: 0.1186"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "only got 1284/1500 samples!\n",
      "only got 1233/1500 samples!\n",
      "only got 1322/1500 samples!\n",
      "only got 1238/1500 samples!\n",
      "only got 733/1500 samples!\n",
      "only got 1262/1500 samples!\n",
      "only got 1412/1500 samples!\n",
      "only got 1171/1500 samples!\n",
      "only got 1426/1500 samples!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 20/372\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:10:33\u001b[0m 12s/step - loss: 0.1170"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "only got 1444/1500 samples!\n",
      "only got 967/1500 samples!\n",
      "only got 1275/1500 samples!\n",
      "only got 1103/1500 samples!\n",
      "only got 1477/1500 samples!\n",
      "only got 1039/1500 samples!\n",
      "only got 1188/1500 samples!\n",
      "only got 1453/1500 samples!\n",
      "only got 1199/1500 samples!\n",
      "only got 1457/1500 samples!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 21/372\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:09:41\u001b[0m 12s/step - loss: 0.1154"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "only got 1331/1500 samples!\n",
      "only got 1194/1500 samples!\n",
      "only got 1378/1500 samples!\n",
      "only got 1466/1500 samples!\n",
      "only got 1265/1500 samples!\n",
      "only got 1227/1500 samples!\n",
      "only got 1161/1500 samples!\n",
      "only got 1091/1500 samples!\n",
      "only got 1399/1500 samples!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 22/372\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:09:04\u001b[0m 12s/step - loss: 0.1140"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "only got 1332/1500 samples!\n",
      "only got 864/1500 samples!\n",
      "only got 1371/1500 samples!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 23/372\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:09:00\u001b[0m 12s/step - loss: 0.1125"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "only got 1484/1500 samples!\n",
      "only got 871/1500 samples!\n",
      "only got 835/1500 samples!\n",
      "only got 1445/1500 samples!\n",
      "only got 1130/1500 samples!\n",
      "only got 1405/1500 samples!\n",
      "only got 1044/1500 samples!\n",
      "only got 1261/1500 samples!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 24/372\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:08:13\u001b[0m 12s/step - loss: 0.1112"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "only got 1345/1500 samples!\n",
      "only got 1205/1500 samples!\n",
      "only got 857/1500 samples!\n",
      "only got 1012/1500 samples!\n",
      "only got 1307/1500 samples!\n",
      "only got 1182/1500 samples!\n",
      "only got 1372/1500 samples!\n",
      "only got 1099/1500 samples!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 25/372\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:07:32\u001b[0m 12s/step - loss: 0.1099"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "only got 1345/1500 samples!\n",
      "only got 1464/1500 samples!\n",
      "only got 1117/1500 samples!\n",
      "only got 1443/1500 samples!\n",
      "only got 1404/1500 samples!\n",
      "only got 1175/1500 samples!\n",
      "only got 1086/1500 samples!\n",
      "only got 997/1500 samples!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 26/372\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:06:59\u001b[0m 12s/step - loss: 0.1086"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "only got 1499/1500 samples!\n",
      "only got 1440/1500 samples!\n",
      "only got 1089/1500 samples!\n",
      "only got 1044/1500 samples!\n",
      "only got 1420/1500 samples!\n",
      "only got 1392/1500 samples!\n",
      "only got 1423/1500 samples!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 27/372\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:06:53\u001b[0m 12s/step - loss: 0.1074"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "only got 1170/1500 samples!\n",
      "only got 1412/1500 samples!\n",
      "only got 1361/1500 samples!\n",
      "only got 1235/1500 samples!\n",
      "only got 1360/1500 samples!\n",
      "only got 1229/1500 samples!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 28/372\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:06:15\u001b[0m 12s/step - loss: 0.1063"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "only got 1183/1500 samples!\n",
      "only got 1144/1500 samples!\n",
      "only got 1160/1500 samples!\n",
      "only got 1207/1500 samples!\n",
      "only got 1347/1500 samples!\n",
      "only got 1302/1500 samples!\n",
      "only got 1160/1500 samples!\n",
      "only got 1430/1500 samples!\n",
      "only got 1480/1500 samples!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 29/372\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:05:42\u001b[0m 11s/step - loss: 0.1052"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "only got 1399/1500 samples!\n",
      "only got 1066/1500 samples!\n",
      "only got 1394/1500 samples!\n",
      "only got 1043/1500 samples!\n",
      "only got 1402/1500 samples!\n",
      "only got 1468/1500 samples!\n",
      "only got 1217/1500 samples!\n",
      "only got 1367/1500 samples!\n",
      "only got 958/1500 samples!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 30/372\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:05:27\u001b[0m 11s/step - loss: 0.1042"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "only got 1123/1500 samples!\n",
      "only got 1201/1500 samples!\n",
      "only got 1311/1500 samples!\n",
      "only got 1438/1500 samples!\n",
      "only got 1157/1500 samples!\n",
      "only got 1120/1500 samples!\n",
      "only got 1099/1500 samples!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 31/372\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:05:04\u001b[0m 11s/step - loss: 0.1032"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "only got 852/1500 samples!\n",
      "only got 857/1500 samples!\n",
      "only got 829/1500 samples!\n",
      "only got 839/1500 samples!\n",
      "only got 1392/1500 samples!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 32/372\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:04:32\u001b[0m 11s/step - loss: 0.1022"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "only got 1173/1500 samples!\n",
      "only got 1166/1500 samples!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 33/372\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:04:02\u001b[0m 11s/step - loss: 0.1013"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "only got 1219/1500 samples!\n",
      "only got 1414/1500 samples!\n",
      "only got 722/1500 samples!\n",
      "only got 1352/1500 samples!\n",
      "only got 1413/1500 samples!\n",
      "only got 1433/1500 samples!\n",
      "only got 1180/1500 samples!\n",
      "only got 1380/1500 samples!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 34/372\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:04:27\u001b[0m 11s/step - loss: 0.1004"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "only got 876/1500 samples!\n",
      "only got 1209/1500 samples!\n",
      "only got 1341/1500 samples!\n",
      "only got 1251/1500 samples!\n",
      "only got 1454/1500 samples!\n",
      "only got 1188/1500 samples!\n",
      "only got 1368/1500 samples!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 51\u001b[0m\n\u001b[1;32m     40\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmean_squared_error\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[39m# for i in range(len(train_data_gen)):\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39m#     print(f\"Iteration: {i}\")\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[39m#     data = train_data_gen[i]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39m#     loss = model.evaluate(inputs, targets, verbose=0)\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[39m#     print(f\"Loss at batch {index}: {loss}\")\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mfit(\n\u001b[1;32m     52\u001b[0m     train_data_gen,\n\u001b[1;32m     53\u001b[0m     epochs\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m,\n\u001b[1;32m     54\u001b[0m     validation_data\u001b[39m=\u001b[39mval_data_gen\n\u001b[1;32m     55\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    118\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:320\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[39mfor\u001b[39;00m step, iterator \u001b[39min\u001b[39;00m epoch_iterator\u001b[39m.\u001b[39menumerate_epoch():\n\u001b[1;32m    319\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m--> 320\u001b[0m     logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_function(iterator)\n\u001b[1;32m    321\u001b[0m     logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pythonify_logs(logs)\n\u001b[1;32m    322\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_end(step, logs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[39m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[39m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[39m=\u001b[39m tracing_compilation\u001b[39m.\u001b[39mcall_function(\n\u001b[1;32m    879\u001b[0m     args, kwds, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_config\n\u001b[1;32m    880\u001b[0m )\n\u001b[1;32m    881\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCreating variables on a non-first call to a function\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39m decorated with tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mbind(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[39m=\u001b[39m function\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[39mreturn\u001b[39;00m function\u001b[39m.\u001b[39m_call_flat(  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     flat_inputs, captured_inputs\u001b[39m=\u001b[39mfunction\u001b[39m.\u001b[39mcaptured_inputs\n\u001b[1;32m    141\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inference_function\u001b[39m.\u001b[39mcall_preflattened(args)\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcall_preflattened\u001b[39m(\u001b[39mself\u001b[39m, args: Sequence[core\u001b[39m.\u001b[39mTensor]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcall_flat(\u001b[39m*\u001b[39margs)\n\u001b[1;32m    217\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mcall_function(\n\u001b[1;32m    252\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname,\n\u001b[1;32m    253\u001b[0m         \u001b[39mlist\u001b[39m(args),\n\u001b[1;32m    254\u001b[0m         \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction_type\u001b[39m.\u001b[39mflat_outputs),\n\u001b[1;32m    255\u001b[0m     )\n\u001b[1;32m    256\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[39mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[39mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mfunction_call_options\u001b[39m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1498\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[1;32m   1499\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1500\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute(\n\u001b[1;32m   1501\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m   1502\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[1;32m   1503\u001b[0m       inputs\u001b[39m=\u001b[39mtensor_inputs,\n\u001b[1;32m   1504\u001b[0m       attrs\u001b[39m=\u001b[39mattrs,\n\u001b[1;32m   1505\u001b[0m       ctx\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m,\n\u001b[1;32m   1506\u001b[0m   )\n\u001b[1;32m   1507\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1508\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1509\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m   1510\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1514\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[1;32m   1515\u001b[0m   )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "img_dir = \"../resized_images/\"\n",
    "img_paths = []\n",
    "\n",
    "# Iterate over the images in the directory, but only add those present in the JSON mapping\n",
    "for root, dirs, files in os.walk(img_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(('.jpg', '.jpeg', '.png')):\n",
    "            img_path = os.path.join(root, file)\n",
    "\n",
    "            # Check if the image path is in the mapping\n",
    "            if img_path in img_to_mod_map:\n",
    "                img_paths.append(img_path)\n",
    "\n",
    "print(f\"Found {len(img_paths)} images.\")\n",
    "\n",
    "train_img_paths, temp_img_paths = train_test_split(img_paths, test_size=0.3, random_state=42)\n",
    "val_img_paths, test_img_paths = train_test_split(temp_img_paths, test_size=0.5, random_state=42)\n",
    "\n",
    "train_data_gen = DataGenerator(train_img_paths, img_to_mod_map, material_properties, batch_size=16, augment=True)\n",
    "val_data_gen = DataGenerator(val_img_paths, img_to_mod_map, material_properties, batch_size=16, augment=False)\n",
    "test_data_gen = DataGenerator(test_img_paths, img_to_mod_map, material_properties, batch_size=16, augment=False)\n",
    "\n",
    "def generator_to_tf_dataset(generator):\n",
    "    output_signature = (\n",
    "        (\n",
    "            tf.TensorSpec(shape=(None, 256, 256, 3), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(None, 12), dtype=tf.float32)\n",
    "        ),\n",
    "        tf.TensorSpec(shape=(None, 1500, 3), dtype=tf.float32)\n",
    "    )\n",
    "    return tf.data.Dataset.from_generator(lambda: generator, output_signature=output_signature)\n",
    "\n",
    "train_dataset = generator_to_tf_dataset(train_data_gen)\n",
    "val_dataset = generator_to_tf_dataset(val_data_gen)\n",
    "test_dataset = generator_to_tf_dataset(test_data_gen)\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "# for i in range(len(train_data_gen)):\n",
    "#     print(f\"Iteration: {i}\")\n",
    "#     data = train_data_gen[i]\n",
    "#     print(f\"Batch {i} processed\")\n",
    "\n",
    "# for index, (inputs, targets) in enumerate(train_data_gen):\n",
    "#     print(f\"Processing batch index: {index}\")\n",
    "#     loss = model.evaluate(inputs, targets, verbose=0)\n",
    "#     print(f\"Loss at batch {index}: {loss}\")\n",
    "\n",
    "history = model.fit(\n",
    "    train_data_gen,\n",
    "    epochs=20,\n",
    "    validation_data=val_data_gen\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mayplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b8110e51c1d4d631a5b9d1ce506b9869acf625cbf303a36efadedb1aaf848f0b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
