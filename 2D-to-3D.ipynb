{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D Image to 3D Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Material Properties From json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import trimesh\n",
    "\n",
    "with open('resized_img_processed_model_mapping.json', 'r') as f:\n",
    "    img_to_mod_map = json.load(f)\n",
    "\n",
    "with open('material_properties.json', 'r') as f:\n",
    "    material_properties = json.load(f)\n",
    "\n",
    "\n",
    "def load_preprocess_img(img_path):\n",
    "    \n",
    "    img = Image.open(img_path)\n",
    "    img_array = np.array(img)\n",
    "    img_array = img_array / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    return img_array\n",
    "\n",
    "def simplify_mesh(mesh, target_vertices=1024):\n",
    "    simplified_mesh = mesh.simplify_quadratic_decimation(target_vertices)\n",
    "    return simplified_mesh\n",
    "\n",
    "def upsample_mesh(mesh, target_vertices=1024):\n",
    "    sampled_points, _ = trimesh.sample.sample_surface_even(mesh, target_vertices)\n",
    "    return sampled_points\n",
    "\n",
    "\n",
    "def load_3d_model(model_path, target_vertices=1024):\n",
    "    mesh = trimesh.load(model_path)\n",
    "\n",
    "    if len(mesh.vertices) > target_vertices:\n",
    "        simplified_mesh = simplify_mesh(mesh, target_vertices)\n",
    "        return simplified_mesh\n",
    "\n",
    "    elif len(mesh.vertices) < target_vertices:\n",
    "        upsampled_mesh = upsample_mesh(mesh, target_vertices)\n",
    "        return upsampled_mesh\n",
    "    \n",
    "    return mesh\n",
    "\n",
    "\n",
    "def get_material_prop(img_path, img_to_mod_map, material_properties):\n",
    "    model_path = img_to_mod_map.get(img_path, None)\n",
    "\n",
    "    if model_path is None:\n",
    "        raise ValueError(f\"No model found for img: {img_path}\")\n",
    "\n",
    "    material_path = model_path.replace('simple_normal_model.obj', 'model.mtl')\n",
    "    material_path = material_path.replace('../model/', '')\n",
    "    materials = material_properties.get(material_path, None)\n",
    "    print(material_path)\n",
    "    if materials is None:\n",
    "        print(f\"No materials found for model: {material_path}. Using default materials\")\n",
    "        materials = {\n",
    "            'Kd': [1.0, 1.0, 1.0],  # Default white diffuse color\n",
    "            'Ks': [0.0, 0.0, 0.0],  # No specular highlights\n",
    "            'Ns': 96.078431,        # Default shininess\n",
    "            'd': 1.0                # Full opacity\n",
    "        }\n",
    "    return materials\n",
    "\n",
    "\n",
    "def normalize_materials(material):\n",
    "    max_shine = 1000\n",
    "\n",
    "    normalized_material = {\n",
    "        'Kd': material.get('Kd', [1.0, 1.0, 1.0]),\n",
    "        'Ks': material.get('Ks', [0.0, 0.0, 0.0]),\n",
    "        'Ns': material.get('Ns', 96.078431) / max_shine,\n",
    "        'd': material.get('d', 1.0)\n",
    "    }\n",
    "\n",
    "    return normalized_material\n",
    "\n",
    "\n",
    "def preprocess_image_with_material(img_path, img_to_mod_map, material_properties):\n",
    "    img = load_preprocess_img(img_path)\n",
    "\n",
    "    model_path = img_to_mod_map.get(img_path)\n",
    "    mesh = load_3d_model(model_path, target_vertices=1024)\n",
    "\n",
    "    materials = get_material_prop(img_path, img_to_mod_map, material_properties)\n",
    "    normalized_materials = normalize_materials(materials)\n",
    "\n",
    "    return img, mesh, normalized_materials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, img_paths, img_to_mod_map, material_properties, batch_size=32, dim=(256, 256, 3), augment=False):\n",
    "        self.img_paths = img_paths\n",
    "        self.img_to_mod_map = img_to_mod_map\n",
    "        self.material_properties = material_properties\n",
    "        self.batch_size = batch_size\n",
    "        self.dim = dim\n",
    "        self.augment = augment\n",
    "\n",
    "        # Image data augmentation\n",
    "        self.image_datagen = ImageDataGenerator(\n",
    "            rotation_range=20,\n",
    "            width_shift_range=0.1,\n",
    "            height_shift_range=0.1,\n",
    "            zoom_range=0.1,\n",
    "            brightness_range=[0.8, 1.2],\n",
    "            fill_mode='nearest'\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.img_paths) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_img_paths = self.img_paths[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        imgs = []\n",
    "        materials = []\n",
    "        meshes = []\n",
    "\n",
    "        for img_path in batch_img_paths:\n",
    "            img = load_preprocess_img(img_path)\n",
    "            img = np.squeeze(img, axis=0)\n",
    "\n",
    "            if self.augment:\n",
    "                img = self.image_datagen.random_transform(img)\n",
    "            \n",
    "            img = np.expand_dims(img, axis=0)\n",
    "            imgs.append(img)\n",
    "\n",
    "            material = get_material_prop(img_path, self.img_to_mod_map, self.material_properties)\n",
    "            normalized_material = normalize_materials(material)\n",
    "            materials.append(normalized_material)\n",
    "\n",
    "            model_path = self.img_to_mod_map.get(img_path)\n",
    "            mesh = load_3d_model(model_path)\n",
    "            \n",
    "            if hasattr(mesh, 'vertices'):\n",
    "                meshes.append(mesh.vertices)\n",
    "            else:\n",
    "                print(f\"Warning: No vertices found in model: {model_path}. Skipping.\")\n",
    "                meshes.append(np.zeros((1024, 3)))\n",
    "\n",
    "        imgs = np.vstack(imgs)\n",
    "        materials = np.array(materials)\n",
    "        meshes = np.array([mesh.vertices for mesh in meshes])\n",
    "\n",
    "        return [imgs, materials], meshes\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.img_paths)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import ResNet50V2\n",
    "\n",
    "image_input = Input(shape=(256, 256, 3), name='image_input')\n",
    "resnet_base = ResNet50V2(weights='imagenet', include_top=False, input_tensor=image_input)\n",
    "\n",
    "x = resnet_base.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "\n",
    "material_input = Input(shape=(4,), name='material_input')\n",
    "material_dense = Dense(64, activation='relu')(material_input)\n",
    "material_dense = Dense(64, activation='relu')(material_dense)\n",
    "\n",
    "combined = Concatenate()([x, material_dense])\n",
    "\n",
    "z = Dense(256, activation='relu')(combined)\n",
    "z = Dense(512, activation='relu')(z)\n",
    "\n",
    "num_vertices = 1024\n",
    "output = Dense(num_vertices * 3, activation='linear', name='output')(z)\n",
    "\n",
    "model = Model(inputs=[image_input, material_input], outputs=output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10000 images.\n",
      "table/IKEA_DOCKSTA/model.mtl\n",
      "Warning: No vertices found in model: ../model/table/IKEA_DOCKSTA/simple_normal_model.obj. Skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "only got 823/1024 samples!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chair/IKEA_INGOLF/model.mtl\n",
      "Warning: No vertices found in model: ../model/chair/IKEA_INGOLF/simple_normal_model.obj. Skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "only got 989/1024 samples!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sofa/IKEA_MANSTAD/model.mtl\n",
      "Warning: No vertices found in model: ../model/sofa/IKEA_MANSTAD/simple_normal_model.obj. Skipping.\n",
      "desk/IKEA_LIATORP/model.mtl\n",
      "Warning: No vertices found in model: ../model/desk/IKEA_LIATORP/simple_normal_model.obj. Skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "only got 761/1024 samples!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chair/IKEA_TOBIAS/model.mtl\n",
      "Warning: No vertices found in model: ../model/chair/IKEA_TOBIAS/simple_normal_model.obj. Skipping.\n",
      "chair/IKEA_POANG_2/model.mtl\n",
      "Warning: No vertices found in model: ../model/chair/IKEA_POANG_2/simple_normal_model.obj. Skipping.\n",
      "chair/IKEA_MARIUS/model.mtl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "only got 783/1024 samples!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No vertices found in model: ../model/chair/IKEA_MARIUS/simple_normal_model.obj. Skipping.\n",
      "bed/IKEA_TROMSO/model.mtl\n",
      "Warning: No vertices found in model: ../model/bed/IKEA_TROMSO/simple_normal_model.obj. Skipping.\n",
      "chair/SS_123/model.mtl\n",
      "No materials found for model: chair/SS_123/model.mtl. Using default materials\n",
      "Warning: No vertices found in model: ../model/chair/SS_123/simple_normal_model.obj. Skipping.\n",
      "sofa/IKEA_SOLSTA_d053e745b565fa391c1b3b2ed8d13bf8/model.mtl\n",
      "Warning: No vertices found in model: ../model/sofa/IKEA_SOLSTA_d053e745b565fa391c1b3b2ed8d13bf8/simple_normal_model.obj. Skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "only got 800/1024 samples!\n",
      "only got 851/1024 samples!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chair/IKEA_EKENAS/model.mtl\n",
      "Warning: No vertices found in model: ../model/chair/IKEA_EKENAS/simple_normal_model.obj. Skipping.\n",
      "chair/IKEA_INGOLF/model.mtl\n",
      "Warning: No vertices found in model: ../model/chair/IKEA_INGOLF/simple_normal_model.obj. Skipping.\n",
      "chair/IKEA_STEFAN/model.mtl\n",
      "Warning: No vertices found in model: ../model/chair/IKEA_STEFAN/simple_normal_model.obj. Skipping.\n",
      "chair/IKEA_MARIUS/model.mtl\n",
      "Warning: No vertices found in model: ../model/chair/IKEA_MARIUS/simple_normal_model.obj. Skipping.\n",
      "bed/IKEA_MALM_2/model.mtl\n",
      "Warning: No vertices found in model: ../model/bed/IKEA_MALM_2/simple_normal_model.obj. Skipping.\n",
      "desk/IKEA_MALM/model.mtl\n",
      "Warning: No vertices found in model: ../model/desk/IKEA_MALM/simple_normal_model.obj. Skipping.\n",
      "chair/IKEA_MARIUS/model.mtl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "only got 642/1024 samples!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No vertices found in model: ../model/chair/IKEA_MARIUS/simple_normal_model.obj. Skipping.\n",
      "table/IKEA_VITTSJO_1/model.mtl\n",
      "Warning: No vertices found in model: ../model/table/IKEA_VITTSJO_1/simple_normal_model.obj. Skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "only got 572/1024 samples!\n",
      "only got 963/1024 samples!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "desk/IKEA_LEKSVIK/model.mtl\n",
      "Warning: No vertices found in model: ../model/desk/IKEA_LEKSVIK/simple_normal_model.obj. Skipping.\n",
      "bookcase/IKEA_BILLY_1/model.mtl\n",
      "Warning: No vertices found in model: ../model/bookcase/IKEA_BILLY_1/simple_normal_model.obj. Skipping.\n",
      "bed/IKEA_HEMNES_2/model.mtl\n",
      "Warning: No vertices found in model: ../model/bed/IKEA_HEMNES_2/simple_normal_model.obj. Skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "only got 556/1024 samples!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bookcase/IKEA_LACK_3/model.mtl\n",
      "Warning: No vertices found in model: ../model/bookcase/IKEA_LACK_3/simple_normal_model.obj. Skipping.\n",
      "chair/IKEA_URBAN/model.mtl\n",
      "Warning: No vertices found in model: ../model/chair/IKEA_URBAN/simple_normal_model.obj. Skipping.\n",
      "table/IKEA_NESNA/model.mtl\n",
      "Warning: No vertices found in model: ../model/table/IKEA_NESNA/simple_normal_model.obj. Skipping.\n",
      "bookcase/IKEA_LACK_2/model.mtl\n",
      "Warning: No vertices found in model: ../model/bookcase/IKEA_LACK_2/simple_normal_model.obj. Skipping.\n",
      "bookcase/IKEA_BILLY_1/model.mtl\n",
      "Warning: No vertices found in model: ../model/bookcase/IKEA_BILLY_1/simple_normal_model.obj. Skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "only got 659/1024 samples!\n",
      "only got 984/1024 samples!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "table/IKEA_DOCKSTA/model.mtl\n",
      "Warning: No vertices found in model: ../model/table/IKEA_DOCKSTA/simple_normal_model.obj. Skipping.\n",
      "table/IKEA_BOKSEL_1/model.mtl\n",
      "Warning: No vertices found in model: ../model/table/IKEA_BOKSEL_1/simple_normal_model.obj. Skipping.\n",
      "tool/SS_7/model.mtl\n",
      "No materials found for model: tool/SS_7/model.mtl. Using default materials\n",
      "Warning: No vertices found in model: ../model/tool/SS_7/simple_normal_model.obj. Skipping.\n",
      "chair/SS_109/model.mtl\n",
      "No materials found for model: chair/SS_109/model.mtl. Using default materials\n",
      "Warning: No vertices found in model: ../model/chair/SS_109/simple_normal_model.obj. Skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "only got 899/1024 samples!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bed/IKEA_NORDLI/model.mtl\n",
      "Warning: No vertices found in model: ../model/bed/IKEA_NORDLI/simple_normal_model.obj. Skipping.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "only got 794/1024 samples!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "desk/IKEA_BESTA_1/model.mtl\n",
      "Warning: No vertices found in model: ../model/desk/IKEA_BESTA_1/simple_normal_model.obj. Skipping.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'vertices'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 22\u001b[0m\n\u001b[1;32m     18\u001b[0m test_data_gen \u001b[39m=\u001b[39m DataGenerator(test_img_paths, img_to_mod_map, material_properties, batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m, augment\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     20\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmean_squared_error\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mfit(\n\u001b[1;32m     23\u001b[0m     train_data_gen,\n\u001b[1;32m     24\u001b[0m     epochs\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m,\n\u001b[1;32m     25\u001b[0m     validation_data\u001b[39m=\u001b[39mval_data_gen\n\u001b[1;32m     26\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[39m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "Cell \u001b[0;32mIn[40], line 58\u001b[0m, in \u001b[0;36mDataGenerator.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     56\u001b[0m imgs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mvstack(imgs)\n\u001b[1;32m     57\u001b[0m materials \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(materials)\n\u001b[0;32m---> 58\u001b[0m meshes \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([mesh\u001b[39m.\u001b[39mvertices \u001b[39mfor\u001b[39;00m mesh \u001b[39min\u001b[39;00m meshes])\n\u001b[1;32m     60\u001b[0m \u001b[39mreturn\u001b[39;00m [imgs, materials], meshes\n",
      "Cell \u001b[0;32mIn[40], line 58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m imgs \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mvstack(imgs)\n\u001b[1;32m     57\u001b[0m materials \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(materials)\n\u001b[0;32m---> 58\u001b[0m meshes \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([mesh\u001b[39m.\u001b[39mvertices \u001b[39mfor\u001b[39;00m mesh \u001b[39min\u001b[39;00m meshes])\n\u001b[1;32m     60\u001b[0m \u001b[39mreturn\u001b[39;00m [imgs, materials], meshes\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'vertices'"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "img_dir = \"../resized_images/\"\n",
    "img_paths = []\n",
    "for root, dirs, files in os.walk(img_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(('.jpg', '.jpeg', '.png')):\n",
    "            img_paths.append(os.path.join(root, file))\n",
    "\n",
    "print(f\"Found {len(img_paths)} images.\")\n",
    "\n",
    "train_img_paths, temp_img_paths = train_test_split(img_paths, test_size=0.3, random_state=42)\n",
    "val_img_paths, test_img_paths = train_test_split(temp_img_paths, test_size=0.5, random_state=42)\n",
    "\n",
    "train_data_gen = DataGenerator(train_img_paths, img_to_mod_map, material_properties, batch_size=32, augment=True)\n",
    "val_data_gen = DataGenerator(val_img_paths, img_to_mod_map, material_properties, batch_size=32, augment=False)\n",
    "test_data_gen = DataGenerator(test_img_paths, img_to_mod_map, material_properties, batch_size=32, augment=False)\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "history = model.fit(\n",
    "    train_data_gen,\n",
    "    epochs=20,\n",
    "    validation_data=val_data_gen\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mayplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b8110e51c1d4d631a5b9d1ce506b9869acf625cbf303a36efadedb1aaf848f0b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
