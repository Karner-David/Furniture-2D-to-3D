{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D Image to 3D Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Material Properties From json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from PIL import Image\n",
    "from PIL import ImageOps\n",
    "import numpy as np\n",
    "import trimesh\n",
    "import open3d as o3d\n",
    "import tensorflow as tf\n",
    "\n",
    "with open('resized_img_processed_model_mapping.json', 'r') as f:\n",
    "    img_to_mod_map = json.load(f)\n",
    "\n",
    "with open('material_properties.json', 'r') as f:\n",
    "    material_properties = json.load(f)\n",
    "\n",
    "\n",
    "def load_preprocess_img(img_path):\n",
    "    \n",
    "    img = Image.open(img_path)\n",
    "\n",
    "    if img.mode != 'RGB':\n",
    "        # print(f\"Converting grayscale to RGB for: {img_path}\")\n",
    "        img = ImageOps.grayscale(img)\n",
    "        img = ImageOps.colorize(img, black=\"black\", white=\"white\")\n",
    "\n",
    "    img_array = np.array(img)\n",
    "    img_array = img_array / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    return img_array\n",
    "\n",
    "def simplify_mesh(mesh, target_vertices=1024):\n",
    "\n",
    "    open3d_mesh = o3d.geometry.TriangleMesh(\n",
    "        vertices=o3d.utility.Vector3dVector(mesh.vertices),\n",
    "        triangles=o3d.utility.Vector3iVector(mesh.faces)\n",
    "    )\n",
    "\n",
    "    simplified_mesh = open3d_mesh.simplify_quadric_decimation(target_vertices)\n",
    "    simplified_trimesh = trimesh.Trimesh(\n",
    "        vertices=np.asarray(simplified_mesh.vertices),\n",
    "        faces=np.asarray(simplified_mesh.triangles)\n",
    "    )\n",
    "\n",
    "    return simplified_trimesh\n",
    "\n",
    "def upsample_mesh(mesh, target_vertices=1024):\n",
    "    sampled_points, _ = trimesh.sample.sample_surface_even(mesh, target_vertices)\n",
    "    if len(sampled_points) < target_vertices:\n",
    "        padding = np.zeros((target_vertices - len(sampled_points), 3))\n",
    "        return np.vstack([sampled_points, padding])\n",
    "    return sampled_points\n",
    "\n",
    "\n",
    "def load_3d_model(model_path, target_vertices=2000):\n",
    "    mesh = trimesh.load(model_path)\n",
    "\n",
    "    # print(f\"In load_3d: len(mesh.vertices): {len(mesh.vertices)}\")\n",
    "    if len(mesh.vertices) > target_vertices:\n",
    "        simplified_mesh = simplify_mesh(mesh, target_vertices)\n",
    "        # print(f\"Simplify mesh: len(mesh.vertices): {len(mesh.vertices)}\")\n",
    "        return simplified_mesh\n",
    "\n",
    "    elif len(mesh.vertices) < target_vertices:\n",
    "        upsampled_mesh = upsample_mesh(mesh, target_vertices)\n",
    "        # print(f\"Upsample mesh: len(mesh.vertices): {len(mesh.vertices)}\")\n",
    "        return upsampled_mesh\n",
    "    \n",
    "    return mesh\n",
    "\n",
    "\n",
    "def get_material_prop(img_path, img_to_mod_map, material_properties):\n",
    "    # print(img_path)\n",
    "    model_path = img_to_mod_map.get(img_path, None)\n",
    "    print(f\"Processing image: {img_path} with mesh: {model_path}\")\n",
    "\n",
    "    if model_path is None:\n",
    "        raise ValueError(f\"No model found for img: {img_path}\")\n",
    "\n",
    "    material_path = model_path.replace('simple_normal_model.obj', 'model.mtl')\n",
    "    material_path = material_path.replace('../model/', '')\n",
    "    materials = material_properties.get(material_path, None)\n",
    "    if materials is None:\n",
    "        raise ValueError(f\"No materials found for model: {material_path}\")\n",
    "    return materials\n",
    "\n",
    "\n",
    "def normalize_materials(material):\n",
    "    max_shine = 1000\n",
    "\n",
    "    normalized_material = {\n",
    "        'Kd': material.get('diffuse', [1.0, 1.0, 1.0]),\n",
    "        'Ks': material.get('specular', [0.0, 0.0, 0.0]),\n",
    "        'Ns': material.get('shininess', 96.078431) / max_shine,\n",
    "        'Ka': material.get('ambient', [0.0, 0.0, 0.0]),\n",
    "        'd': material.get('transparency', 1.0),\n",
    "        'illumination': material.get('illumination', 2)\n",
    "    }\n",
    "\n",
    "    # Flatten the normalized material into a list for easier processing\n",
    "    flattened_material = (\n",
    "        normalized_material['Kd'] + \n",
    "        normalized_material['Ks'] + \n",
    "        [normalized_material['Ns']] + \n",
    "        normalized_material['Ka'] + \n",
    "        [normalized_material['d'], normalized_material['illumination']]\n",
    "    )\n",
    "    \n",
    "    return flattened_material\n",
    "\n",
    "\n",
    "def preprocess_image_with_material(img_path, img_to_mod_map, material_properties):\n",
    "    img = load_preprocess_img(img_path)\n",
    "\n",
    "    model_path = img_to_mod_map.get(img_path)\n",
    "    mesh = load_3d_model(model_path, target_vertices=2000)\n",
    "\n",
    "    materials = get_material_prop(img_path, img_to_mod_map, material_properties)\n",
    "    normalized_materials = normalize_materials(materials)\n",
    "\n",
    "    return img, mesh, normalized_materials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, img_paths, img_to_mod_map, material_properties, batch_size=16, dim=(256, 256, 3), augment=False):\n",
    "        self.img_paths = img_paths\n",
    "        self.img_to_mod_map = img_to_mod_map\n",
    "        self.material_properties = material_properties\n",
    "        self.batch_size = batch_size\n",
    "        self.dim = dim\n",
    "        self.augment = augment\n",
    "\n",
    "        # Image data augmentation\n",
    "        self.image_datagen = ImageDataGenerator(\n",
    "            rotation_range=20,\n",
    "            width_shift_range=0.1,\n",
    "            height_shift_range=0.1,\n",
    "            zoom_range=0.1,\n",
    "            brightness_range=[0.8, 1.2],\n",
    "            fill_mode='nearest'\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.floor(len(self.img_paths) / self.batch_size))\n",
    "\n",
    "    def pad_or_trunc_mesh(self, mesh, target_vertices=2000):\n",
    "        vertices = np.array(mesh)\n",
    "        # print(f\"Vertices shape before padding: {vertices.shape}\")\n",
    "\n",
    "        if vertices.shape[0] > target_vertices:\n",
    "            return vertices[:target_vertices, :]\n",
    "        elif vertices.shape[0] < target_vertices:\n",
    "            padding = np.zeros((target_vertices - vertices.shape[0], vertices.shape[1]))\n",
    "            return np.vstack([vertices, padding])\n",
    "        else:\n",
    "            return vertices\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        batch_img_paths = self.img_paths[index * self.batch_size:(index + 1) * self.batch_size]\n",
    "        if len(batch_img_paths) == 0:\n",
    "            raise ValueError(f\"Batch {index} is empty. Skipping...\")\n",
    "\n",
    "        print(f\"Batch index {index} size: {len(batch_img_paths)}\")\n",
    "\n",
    "        imgs = []\n",
    "        materials = []\n",
    "        meshes = []\n",
    "\n",
    "        for img_path in batch_img_paths:\n",
    "            img = load_preprocess_img(img_path)\n",
    "\n",
    "            # Only squeeze if the image has 4 dimensions\n",
    "            # print(img.shape)\n",
    "            if len(img.shape) == 4 and img.shape[0] == 1:\n",
    "                img = np.squeeze(img, axis=0)  # Remove batch dimension if present\n",
    "\n",
    "            if len(img.shape) == 2: \n",
    "                # print(f\"Converting grayscale to RGB for: {img_path}\")\n",
    "                img = np.stack([img] * 3, axis=-1) \n",
    "\n",
    "            if self.augment:\n",
    "                # print(img.shape)\n",
    "                img = self.image_datagen.random_transform(img)\n",
    "            \n",
    "            img = np.expand_dims(img, axis=0)\n",
    "            imgs.append(img)\n",
    "\n",
    "            material = get_material_prop(img_path, self.img_to_mod_map, self.material_properties)\n",
    "            normalized_material = normalize_materials(material)\n",
    "            materials.append(normalized_material)\n",
    "\n",
    "            model_path = self.img_to_mod_map.get(img_path)\n",
    "            mesh = load_3d_model(model_path)\n",
    "            # print(f\"Mesh type: {type(mesh)} for model: {model_path}\")\n",
    "\n",
    "            if index == 8:\n",
    "                print(f\"Batch 8 - Image Path: {img_path}, Model Path: {model_path}\")\n",
    "\n",
    "            # Additional logging for mesh loading issues\n",
    "            if hasattr(mesh, 'vertices'):\n",
    "                vertices_before = len(mesh.vertices)\n",
    "                print(f\"Batch {index} - Image Path: {img_path}, Model Path: {model_path}\")\n",
    "                # print(f\"(If hasattr(mesh, 'vertices') | Vertices shape before padding: {vertices_before}\")\n",
    "                # if vertices_before < 1024:\n",
    "                #     # print(f\"Warning: Mesh for {model_path} has fewer vertices ({vertices_before}) than required. Adding padding.\")\n",
    "                # if vertices_before < 500:  # Log very small meshes\n",
    "                #     # print(f\"Warning: Mesh for {model_path} has abnormally few vertices ({vertices_before}).\")\n",
    "                padded_mesh = self.pad_or_trunc_mesh(mesh.vertices)\n",
    "            elif isinstance(mesh, np.ndarray) or isinstance(mesh, trimesh.caching.TrackedArray):\n",
    "                vertices_before = len(mesh)\n",
    "                print(f\"Batch {index} - Image Path: {img_path}, Model Path: {model_path}\")\n",
    "                # print(f\"(If isinstance(mesh, np.ndarray)... | Vertices shape before padding: {vertices_before}\")\n",
    "                # if vertices_before < 500:  # Log very small meshes\n",
    "                #     print(f\"Warning: Mesh for {model_path} has abnormally few vertices ({vertices_before}).\")\n",
    "                # if vertices_before < 1024:\n",
    "                #     print(f\"Warning: Mesh for {model_path} has fewer vertices ({vertices_before}) than required. Adding padding.\")\n",
    "                padded_mesh = self.pad_or_trunc_mesh(mesh)\n",
    "            else:\n",
    "                print(f\"Warning: No vertices found in model: {model_path}. Skipping.\")\n",
    "                padded_mesh = np.zeros((1024, 3))\n",
    "\n",
    "\n",
    "            # if index == 8:\n",
    "            #     print(f\"Batch 8 - Padded Mesh Shape: {padded_mesh.shape}\")\n",
    "                \n",
    "            # print(f\"Final mesh shape: {padded_mesh.shape}\")\n",
    "            assert padded_mesh.shape[0] == 2000, f\"Unexpected vertex count: {padded_mesh.shape[0]} for mesh in batch {index}\"\n",
    "            meshes.append(padded_mesh)\n",
    "\n",
    "        imgs = np.vstack(imgs)\n",
    "        materials = np.array(materials, dtype=np.float32)\n",
    "        meshes = np.array(meshes, dtype=np.float32)\n",
    "\n",
    "        imgs_tensor = tf.convert_to_tensor(imgs, dtype=tf.float32)\n",
    "        materials_tensor = tf.convert_to_tensor(materials, dtype=tf.float32)\n",
    "        meshes_tensor = tf.convert_to_tensor(meshes, dtype=tf.float32)\n",
    "\n",
    "        return (imgs_tensor, materials_tensor), meshes_tensor\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.img_paths)\n",
    "        # print(\"Shuffled\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D, Concatenate, Reshape, BatchNormalization, ReLU\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications import ResNet50V2\n",
    "\n",
    "image_input = Input(shape=(256, 256, 3), name='image_input')\n",
    "resnet_base = ResNet50V2(weights='imagenet', include_top=False, input_tensor=image_input)\n",
    "\n",
    "x = resnet_base.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(512)(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = ReLU()(x)\n",
    "\n",
    "material_input = Input(shape=(12,), name='material_input')\n",
    "material_dense = Dense(64)(material_input)\n",
    "material_dense = BatchNormalization()(material_dense)\n",
    "material_dense = ReLU()(material_dense)\n",
    "\n",
    "combined = Concatenate()([x, material_dense])\n",
    "\n",
    "z = Dense(256)(combined)\n",
    "z = BatchNormalization()(z)\n",
    "z = ReLU()(z)\n",
    "z = Dense(512, activation='relu')(z)\n",
    "z = BatchNormalization()(z)\n",
    "z = ReLU()(z)\n",
    "\n",
    "output = Dense(2000 * 3, activation='linear', name='output')(z)\n",
    "output_reshaped = Reshape((2000, 3))(output)\n",
    "\n",
    "model = Model(inputs=[image_input, material_input], outputs=output_reshaped)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8521 images.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "only got 1600/2000 samples!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch index 0 size: 16\n",
      "Processing image: ../resized_images/bed/0229.jpg with mesh: ../model/bed/IKEA_FJELLSE_1/model_simple_normal.obj\n",
      "Batch 0 - Image Path: ../resized_images/bed/0229.jpg, Model Path: ../model/bed/IKEA_FJELLSE_1/model_simple_normal.obj\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "only got 1893/2000 samples!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image: ../resized_images/chair/1806.jpg with mesh: ../model/chair/IKEA_EKTORP_1/model_simple_normal.obj\n",
      "Batch 0 - Image Path: ../resized_images/chair/1806.jpg, Model Path: ../model/chair/IKEA_EKTORP_1/model_simple_normal.obj\n",
      "Processing image: ../resized_images/chair/2528.jpg with mesh: ../model/chair/IKEA_MARIUS/model_simple_normal.obj\n",
      "Batch 0 - Image Path: ../resized_images/chair/2528.jpg, Model Path: ../model/chair/IKEA_MARIUS/model_simple_normal.obj\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "only got 1549/2000 samples!\n",
      "only got 1642/2000 samples!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image: ../resized_images/bookcase/0151.jpg with mesh: ../model/bookcase/IKEA_EXPEDIT_3/model_simple_normal.obj\n",
      "Batch 0 - Image Path: ../resized_images/bookcase/0151.jpg, Model Path: ../model/bookcase/IKEA_EXPEDIT_3/model_simple_normal.obj\n",
      "Processing image: ../resized_images/sofa/0340.jpg with mesh: ../model/sofa/IKEA_EKTORP_2/model_simple_normal.obj\n",
      "Batch 0 - Image Path: ../resized_images/sofa/0340.jpg, Model Path: ../model/sofa/IKEA_EKTORP_2/model_simple_normal.obj\n",
      "Processing image: ../resized_images/chair/3150.jpg with mesh: ../model/chair/IKEA_REIDAR/model_simple_normal.obj\n",
      "Batch 0 - Image Path: ../resized_images/chair/3150.jpg, Model Path: ../model/chair/IKEA_REIDAR/model_simple_normal.obj\n",
      "Processing image: ../resized_images/chair/2152.jpg with mesh: ../model/chair/IKEA_JOKKMOKK/model_simple_normal.obj\n",
      "Batch 0 - Image Path: ../resized_images/chair/2152.jpg, Model Path: ../model/chair/IKEA_JOKKMOKK/model_simple_normal.obj\n",
      "Processing image: ../resized_images/bookcase/0269.jpg with mesh: ../model/bookcase/IKEA_LACK_3/model_simple_normal.obj\n",
      "Batch 0 - Image Path: ../resized_images/bookcase/0269.jpg, Model Path: ../model/bookcase/IKEA_LACK_3/model_simple_normal.obj\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "only got 1966/2000 samples!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image: ../resized_images/chair/1752.jpg with mesh: ../model/chair/IKEA_BORJE/model_simple_normal.obj\n",
      "Batch 0 - Image Path: ../resized_images/chair/1752.jpg, Model Path: ../model/chair/IKEA_BORJE/model_simple_normal.obj\n",
      "Processing image: ../resized_images/table/0847.jpg with mesh: ../model/table/IKEA_ISALA_2/model_simple_normal.obj\n",
      "Batch 0 - Image Path: ../resized_images/table/0847.jpg, Model Path: ../model/table/IKEA_ISALA_2/model_simple_normal.obj\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 46\u001b[0m\n\u001b[1;32m     40\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mmean_squared_error\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     41\u001b[0m \u001b[39m# for i in range(len(train_data_gen)):\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39m#     print(f\"Iteration: {i}\")\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[39m#     data = train_data_gen[i]\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[39m#     print(f\"Batch {i} processed\")\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m \u001b[39mfor\u001b[39;00m index, (inputs, targets) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_data_gen):\n\u001b[1;32m     47\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mProcessing batch index: \u001b[39m\u001b[39m{\u001b[39;00mindex\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     48\u001b[0m     loss \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mevaluate(inputs, targets, verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "Cell \u001b[0;32mIn[22], line 64\u001b[0m, in \u001b[0;36mDataGenerator.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     60\u001b[0m     img \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mstack([img] \u001b[39m*\u001b[39m \u001b[39m3\u001b[39m, axis\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \n\u001b[1;32m     62\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maugment:\n\u001b[1;32m     63\u001b[0m     \u001b[39m# print(img.shape)\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_datagen\u001b[39m.\u001b[39mrandom_transform(img)\n\u001b[1;32m     66\u001b[0m img \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexpand_dims(img, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     67\u001b[0m imgs\u001b[39m.\u001b[39mappend(img)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/legacy/preprocessing/image.py:1460\u001b[0m, in \u001b[0;36mImageDataGenerator.random_transform\u001b[0;34m(self, x, seed)\u001b[0m\n\u001b[1;32m   1450\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Applies a random transformation to an image.\u001b[39;00m\n\u001b[1;32m   1451\u001b[0m \n\u001b[1;32m   1452\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1457\u001b[0m \u001b[39m    A randomly transformed version of the input (same shape).\u001b[39;00m\n\u001b[1;32m   1458\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1459\u001b[0m params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_random_transform(x\u001b[39m.\u001b[39mshape, seed)\n\u001b[0;32m-> 1460\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_transform(x, params)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/legacy/preprocessing/image.py:1413\u001b[0m, in \u001b[0;36mImageDataGenerator.apply_transform\u001b[0;34m(self, x, transform_parameters)\u001b[0m\n\u001b[1;32m   1410\u001b[0m img_col_axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcol_axis \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   1411\u001b[0m img_channel_axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mchannel_axis \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m-> 1413\u001b[0m x \u001b[39m=\u001b[39m apply_affine_transform(\n\u001b[1;32m   1414\u001b[0m     x,\n\u001b[1;32m   1415\u001b[0m     transform_parameters\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtheta\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m0\u001b[39m),\n\u001b[1;32m   1416\u001b[0m     transform_parameters\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtx\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m0\u001b[39m),\n\u001b[1;32m   1417\u001b[0m     transform_parameters\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mty\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m0\u001b[39m),\n\u001b[1;32m   1418\u001b[0m     transform_parameters\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mshear\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m0\u001b[39m),\n\u001b[1;32m   1419\u001b[0m     transform_parameters\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mzx\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m1\u001b[39m),\n\u001b[1;32m   1420\u001b[0m     transform_parameters\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mzy\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m1\u001b[39m),\n\u001b[1;32m   1421\u001b[0m     row_axis\u001b[39m=\u001b[39mimg_row_axis,\n\u001b[1;32m   1422\u001b[0m     col_axis\u001b[39m=\u001b[39mimg_col_axis,\n\u001b[1;32m   1423\u001b[0m     channel_axis\u001b[39m=\u001b[39mimg_channel_axis,\n\u001b[1;32m   1424\u001b[0m     fill_mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfill_mode,\n\u001b[1;32m   1425\u001b[0m     cval\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcval,\n\u001b[1;32m   1426\u001b[0m     order\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minterpolation_order,\n\u001b[1;32m   1427\u001b[0m )\n\u001b[1;32m   1429\u001b[0m \u001b[39mif\u001b[39;00m transform_parameters\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mchannel_shift_intensity\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1430\u001b[0m     x \u001b[39m=\u001b[39m apply_channel_shift(\n\u001b[1;32m   1431\u001b[0m         x,\n\u001b[1;32m   1432\u001b[0m         transform_parameters[\u001b[39m\"\u001b[39m\u001b[39mchannel_shift_intensity\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m   1433\u001b[0m         img_channel_axis,\n\u001b[1;32m   1434\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/legacy/preprocessing/image.py:1879\u001b[0m, in \u001b[0;36mapply_affine_transform\u001b[0;34m(x, theta, tx, ty, shear, zx, zy, row_axis, col_axis, channel_axis, fill_mode, cval, order)\u001b[0m\n\u001b[1;32m   1876\u001b[0m final_affine_matrix \u001b[39m=\u001b[39m transform_matrix[:\u001b[39m2\u001b[39m, :\u001b[39m2\u001b[39m]\n\u001b[1;32m   1877\u001b[0m final_offset \u001b[39m=\u001b[39m transform_matrix[:\u001b[39m2\u001b[39m, \u001b[39m2\u001b[39m]\n\u001b[0;32m-> 1879\u001b[0m channel_images \u001b[39m=\u001b[39m [\n\u001b[1;32m   1880\u001b[0m     scipy\u001b[39m.\u001b[39mndimage\u001b[39m.\u001b[39minterpolation\u001b[39m.\u001b[39maffine_transform(\n\u001b[1;32m   1881\u001b[0m         x_channel,\n\u001b[1;32m   1882\u001b[0m         final_affine_matrix,\n\u001b[1;32m   1883\u001b[0m         final_offset,\n\u001b[1;32m   1884\u001b[0m         order\u001b[39m=\u001b[39morder,\n\u001b[1;32m   1885\u001b[0m         mode\u001b[39m=\u001b[39mfill_mode,\n\u001b[1;32m   1886\u001b[0m         cval\u001b[39m=\u001b[39mcval,\n\u001b[1;32m   1887\u001b[0m     )\n\u001b[1;32m   1888\u001b[0m     \u001b[39mfor\u001b[39;00m x_channel \u001b[39min\u001b[39;00m x\n\u001b[1;32m   1889\u001b[0m ]\n\u001b[1;32m   1890\u001b[0m x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mstack(channel_images, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m   1891\u001b[0m x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrollaxis(x, \u001b[39m0\u001b[39m, channel_axis \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/legacy/preprocessing/image.py:1880\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1876\u001b[0m final_affine_matrix \u001b[39m=\u001b[39m transform_matrix[:\u001b[39m2\u001b[39m, :\u001b[39m2\u001b[39m]\n\u001b[1;32m   1877\u001b[0m final_offset \u001b[39m=\u001b[39m transform_matrix[:\u001b[39m2\u001b[39m, \u001b[39m2\u001b[39m]\n\u001b[1;32m   1879\u001b[0m channel_images \u001b[39m=\u001b[39m [\n\u001b[0;32m-> 1880\u001b[0m     scipy\u001b[39m.\u001b[39mndimage\u001b[39m.\u001b[39minterpolation\u001b[39m.\u001b[39maffine_transform(\n\u001b[1;32m   1881\u001b[0m         x_channel,\n\u001b[1;32m   1882\u001b[0m         final_affine_matrix,\n\u001b[1;32m   1883\u001b[0m         final_offset,\n\u001b[1;32m   1884\u001b[0m         order\u001b[39m=\u001b[39morder,\n\u001b[1;32m   1885\u001b[0m         mode\u001b[39m=\u001b[39mfill_mode,\n\u001b[1;32m   1886\u001b[0m         cval\u001b[39m=\u001b[39mcval,\n\u001b[1;32m   1887\u001b[0m     )\n\u001b[1;32m   1888\u001b[0m     \u001b[39mfor\u001b[39;00m x_channel \u001b[39min\u001b[39;00m x\n\u001b[1;32m   1889\u001b[0m ]\n\u001b[1;32m   1890\u001b[0m x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mstack(channel_images, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m   1891\u001b[0m x \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrollaxis(x, \u001b[39m0\u001b[39m, channel_axis \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/scipy/ndimage/_interpolation.py:614\u001b[0m, in \u001b[0;36maffine_transform\u001b[0;34m(input, matrix, offset, output_shape, output, order, mode, cval, prefilter)\u001b[0m\n\u001b[1;32m    611\u001b[0m     _nd_image\u001b[39m.\u001b[39mzoom_shift(filtered, matrix, offset\u001b[39m/\u001b[39mmatrix, output, order,\n\u001b[1;32m    612\u001b[0m                          mode, cval, npad, \u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    613\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 614\u001b[0m     _nd_image\u001b[39m.\u001b[39mgeometric_transform(filtered, \u001b[39mNone\u001b[39;00m, \u001b[39mNone\u001b[39;00m, matrix, offset,\n\u001b[1;32m    615\u001b[0m                                   output, order, mode, cval, npad, \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    616\u001b[0m                                   \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    617\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "\n",
    "img_dir = \"../resized_images/\"\n",
    "img_paths = []\n",
    "\n",
    "# Iterate over the images in the directory, but only add those present in the JSON mapping\n",
    "for root, dirs, files in os.walk(img_dir):\n",
    "    for file in files:\n",
    "        if file.endswith(('.jpg', '.jpeg', '.png')):\n",
    "            img_path = os.path.join(root, file)\n",
    "\n",
    "            # Check if the image path is in the mapping\n",
    "            if img_path in img_to_mod_map:\n",
    "                img_paths.append(img_path)\n",
    "\n",
    "print(f\"Found {len(img_paths)} images.\")\n",
    "\n",
    "train_img_paths, temp_img_paths = train_test_split(img_paths, test_size=0.3, random_state=42)\n",
    "val_img_paths, test_img_paths = train_test_split(temp_img_paths, test_size=0.5, random_state=42)\n",
    "\n",
    "train_data_gen = DataGenerator(train_img_paths, img_to_mod_map, material_properties, batch_size=16, augment=True)\n",
    "val_data_gen = DataGenerator(val_img_paths, img_to_mod_map, material_properties, batch_size=16, augment=False)\n",
    "test_data_gen = DataGenerator(test_img_paths, img_to_mod_map, material_properties, batch_size=16, augment=False)\n",
    "\n",
    "def generator_to_tf_dataset(generator):\n",
    "    output_signature = (\n",
    "        (\n",
    "            tf.TensorSpec(shape=(None, 256, 256, 3), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(None, 12), dtype=tf.float32)\n",
    "        ),\n",
    "        tf.TensorSpec(shape=(None, 2000, 3), dtype=tf.float32)\n",
    "    )\n",
    "    return tf.data.Dataset.from_generator(lambda: generator, output_signature=output_signature)\n",
    "\n",
    "train_dataset = generator_to_tf_dataset(train_data_gen)\n",
    "val_dataset = generator_to_tf_dataset(val_data_gen)\n",
    "test_dataset = generator_to_tf_dataset(test_data_gen)\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "# for i in range(len(train_data_gen)):\n",
    "#     print(f\"Iteration: {i}\")\n",
    "#     data = train_data_gen[i]\n",
    "#     print(f\"Batch {i} processed\")\n",
    "\n",
    "# for index, (inputs, targets) in enumerate(train_data_gen):\n",
    "#     print(f\"Processing batch index: {index}\")\n",
    "#     loss = model.evaluate(inputs, targets, verbose=0)\n",
    "#     print(f\"Loss at batch {index}: {loss}\")\n",
    "\n",
    "history = model.fit(\n",
    "    train_data_gen,\n",
    "    epochs=20,\n",
    "    validation_data=val_data_gen\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mayplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b8110e51c1d4d631a5b9d1ce506b9869acf625cbf303a36efadedb1aaf848f0b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
